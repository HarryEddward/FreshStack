from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# Cargar la documentaci√≥n desde el directorio relativo
documents = SimpleDirectoryReader("../../docs").load_data()

# Modelo local con Ollama
llm = Ollama(model="codellama")

# Configurar modelo de embeddings local
embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Crear √≠ndice y motor de b√∫squeda
index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)
query_engine = index.as_query_engine(llm=llm)

# Bucle interactivo
while True:
    question = input("‚ùì Pregunta a tu documentaci√≥n (o escribe 'salir'): ")
    if question.lower() in ["salir", "exit", "q"]:
        break
    response = query_engine.query(question)
    print("\nüß† Respuesta:\n", response)